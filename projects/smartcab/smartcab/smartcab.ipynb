{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Reinforcement Learning\n",
    "## Project 4: Train a Smartcab How to Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Basic Driving Agent\n",
    "\n",
    "To begin, your only task is to get the smartcab to move around in the environment. At this point, you will not be concerned with any sort of optimal driving policy. Note that the driving agent is given the following information at each intersection:\n",
    "\n",
    "The next waypoint location relative to its current location and heading.\n",
    "\n",
    "+ The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "\n",
    "+ The current time left from the allotted deadline.\n",
    "\n",
    "To complete this task, simply have your driving agent choose a random action from the set of possible actions (None, 'forward', 'left', 'right') at each intersection, disregarding the input information above. Set the simulation deadline enforcement, enforce_deadline to False and observe how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  \n",
    "        # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        # TODO: Update state\n",
    "        \n",
    "        # TODO: Select action according to your policy\n",
    "        action = random.choice([None, 'forward', 'left', 'right'])\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "\n",
    "        # print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(deadline, inputs, action, reward)  # [debug]\n",
    "\n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "\n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(LearningAgent)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=True)  # specify agent to track\n",
    "    # NOTE: You can set enforce_deadline=False while debugging to allow longer trials\n",
    "\n",
    "    # Now simulate it\n",
    "    sim = Simulator(e, update_delay=0, display=False)  # create simulator (uses pygame when display=True, if available)\n",
    "    # NOTE: To speed up simulation, reduce update_delay and/or set display=False\n",
    "\n",
    "    sim.run(n_trials=100)  # run for a specified number of trials\n",
    "    # NOTE: To quit midway, press Esc or close pygame window, or hit Ctrl+C on the command-line\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: \n",
    "\n",
    "Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "\n",
    "The smartcab does pretty much what you expect - it behaves randomly. It would occasionally stop for no reason, run lights and completely discregard traffic. Sometimes it would do the right thing and in very rare cases make it to the destination.\n",
    "\n",
    "There are 3 other agents that exist simultaneously in the world. The observed rewards are as follows:\n",
    "+ Violating a traffic rule: -1.0\n",
    "+ No violation but going in a direction other than specified by the planner: -0.5\n",
    "+ No violation and going in the right direction: 2.0\n",
    "+ No violation, right direction and reaching the destination: 12.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inform the Driving Agent\n",
    "\n",
    "Now that your driving agent is capable of moving around in the environment, your next task is to identify a set of states that are appropriate for modeling the smartcab and environment. The main source of state variables are the current inputs at the intersection, but not all may require representation. You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, process the inputs and update the agent's current state using the self.state variable. Continue with the simulation deadline enforcement enforce_deadline being set to False, and observe how your driving agent now reports the change in state as the simulation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        # TODO: Update state\n",
    "        self.state = inputs\n",
    "        self.state['waypoint'] = self.next_waypoint\n",
    "        \n",
    "        # TODO: Select action according to your policy\n",
    "        action = random.choice([None, 'forward', 'left', 'right'])\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "\n",
    "        # print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(deadline, inputs, action, reward)  # [debug]\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: \n",
    "\n",
    "What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "**OPTIONAL**: \n",
    "\n",
    "How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "\n",
    "The identified state consists of all the inputs plus the next waypoint. The following are its parameters:\n",
    "+ light\n",
    "+ oncoming\n",
    "+ right\n",
    "+ left\n",
    "+ waypoint\n",
    "\n",
    "Generally speaking, our problem is to get our smartcab to a specified location in our grid world. Normally, this would be much more complicated. However, pathing has already been solved for us by the planner module as it already tells us where the next waypoint is. This also effectively let's us bypass the problem of localization, so we don't have to worry about sensing and ascertaining where our smartcab is exactly. What we do have to worry about is the conditions of its immediate vicinity (the inputs) and where it must go next (next waypoint). So the inputs plus the waypoint is all we need really to take an action.\n",
    "\n",
    "Each parameter can take on the following values:\n",
    "+ light ['green', 'red']\n",
    "+ oncoming, right, left [ None, 'left', 'right']\n",
    "+ waypoint [ None, 'left', 'right']\n",
    "\n",
    "Therefore, altogether there are 2x3x3x3x3 = 162 total states. It's a small number compared to possible events that can occur in a real world environment. If we were to take account different vehicles, pedestrians, road signs, weather, road elevation, etc., it would certainly be a lot more than 162 states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Q-Learning Driving Agent\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, based on the Q-values for the current state and action. Each action taken by the smartcab will produce a reward which depends on the state of the environment. The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, set the simulation deadline enforcement enforce_deadline to True. Run the simulation and observe how the smartcab moves about the environment in each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "Q_INITIAL = 2\n",
    "ALPHA = 1\n",
    "\n",
    "class hashabledict(dict):\n",
    "    def __hash__(self):\n",
    "        return hash(frozenset(self.iteritems()))\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.policy = {}\n",
    "        self.alpha = ALPHA\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        # TODO: Update state\n",
    "        self.state = inputs\n",
    "        self.state['waypoint'] = self.next_waypoint\n",
    "\n",
    "        # TODO: Select action according to your policy\n",
    "        state_hash = hashabledict(frozenset(self.state.iteritems()))\n",
    "        q = self.policy.get(state_hash, {\n",
    "            None: Q_INITIAL,\n",
    "            'forward': Q_INITIAL,\n",
    "            'left': Q_INITIAL,\n",
    "            'right': Q_INITIAL,\n",
    "        })\n",
    "        action = max(q.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "        q[action] = (1 - self.alpha) * q[action] + self.alpha * reward\n",
    "        self.policy[state_hash] = q\n",
    "        \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: \n",
    "\n",
    "What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "\n",
    "The smartcab's behavior would start out random but would slowly correct itself once negative rewards were encountered. For example, after it runs a stop light and receives a negative reward, it would stop doing that and try something else the next time around. This is because the Q-value of that state-action pair is degraded and is seen as a less appealing option by the smartcab.\n",
    "\n",
    "Interestingly, the cab encounters more common cases first (usually without traffic) and therefore has Q-values for those states-action pairs first. It can get confused when it encounters an entirely new state as the actions for that state will always start out with uniform Q-values. For example, even though it ran a stop with no traffic before and got a negative reward, if there was suddenly any traffic in any direction, it's possible that it will run the stop again so it can seemingly make the same mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Q-Learning Driving Agent\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, the smartcab is able to reach the destination within the allotted time safely and efficiently. Parameters in the Q-Learning algorithm, such as the learning rate (alpha), the discount factor (gamma) and the exploration rate (epsilon) all contribute to the driving agent’s ability to learn the best action for each state. To improve on the success of your smartcab:\n",
    "\n",
    "+ Set the number of trials, n_trials, in the simulation to 100.\n",
    "+ Run the simulation with the deadline enforcement enforce_deadline set to True (you will need to reduce the update delay update_delay and set the display to False).\n",
    "+ Observe the driving agent’s learning and smartcab’s success rate, particularly during the later trials.\n",
    "+ Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha, q_initial):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'red'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.policy = {}\n",
    "        self.trip_log = []\n",
    "        self.trip = None\n",
    "        self.alpha = alpha\n",
    "        self.q_initial = q_initial\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "        if self.trip:\n",
    "            self.trip_log.append(self.trip)\n",
    "        self.trip = {}\n",
    "        self.trip['Deadline'] = self.env.get_deadline(self)\n",
    "        self.trip['Reward'] = 0\n",
    "        self.trip['Penalty'] = 0\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        # TODO: Update state\n",
    "        self.state = inputs\n",
    "        self.state['waypoint'] = self.next_waypoint\n",
    "\n",
    "        # TODO: Select action according to your policy\n",
    "        state_hash = hashabledict(frozenset(self.state.iteritems()))\n",
    "        q = self.policy.get(state_hash, {\n",
    "            None: self.q_initial,\n",
    "            'forward': self.q_initial,\n",
    "            'left': self.q_initial,\n",
    "            'right': self.q_initial,\n",
    "        })\n",
    "        action = max(q.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "\n",
    "        # Update trip stats\n",
    "        self.trip['Reward'] += reward\n",
    "        self.trip['Remaining'] = self.env.get_deadline(self)\n",
    "        self.trip['Success'] = self.planner.next_waypoint() == None\n",
    "        if reward < 0: self.trip['Penalty'] += reward\n",
    "\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "        q[action] = (1 - self.alpha) * q[action] + self.alpha * reward\n",
    "        self.policy[state_hash] = q\n",
    "\n",
    "        # print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(deadline, inputs, action, reward)  # [debug]\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "\n",
    "    record = []\n",
    "    for q_initial in [0, 2, 10]:\n",
    "        for alpha in range(1, 6):\n",
    "            # Set up environment and agent\n",
    "            e = Environment()  # create environment (also adds some dummy traffic)\n",
    "            a = e.create_agent(LearningAgent, alpha * 0.2, q_initial)  # create agent\n",
    "            e.set_primary_agent(a, enforce_deadline=True)  # specify agent to track\n",
    "            # NOTE: You can set enforce_deadline=False while debugging to allow longer trials\n",
    "\n",
    "            # Now simulate it\n",
    "            sim = Simulator(e, update_delay=0, display=False)  # create simulator (uses pygame when display=True, if available)\n",
    "            # NOTE: To speed up simulation, reduce update_delay and/or set display=False\n",
    "\n",
    "            sim.run(n_trials=100)  # run for a specified number of trials\n",
    "            # NOTE: To quit midway, press Esc or close pygame window, or hit Ctrl+C on the command-line\n",
    "\n",
    "            a.reset()\n",
    "            trip_log = pd.DataFrame(a.trip_log)\n",
    "            # trip_log['Used'] = trip_log['Deadline'] - trip_log['Remaining']\n",
    "            trip_log['Efficiency'] = trip_log['Remaining'] / trip_log['Deadline'] * 100\n",
    "            record.append({\n",
    "                'Success Rate': trip_log[trip_log.Success == True].shape[0],\n",
    "                'Alpha': alpha * 0.2,\n",
    "                'Q Initial': q_initial,\n",
    "                'Efficiency': trip_log['Efficiency'].mean(),\n",
    "                'Ave Reward': trip_log['Reward'].mean(),\n",
    "                'Ave Penalty': trip_log['Penalty'].mean(),\n",
    "            });\n",
    "\n",
    "    return pd.DataFrame(record)\n",
    "\n",
    "record = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: \n",
    "\n",
    "Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "\n",
    "I have chosen to implement the 'optimistic' version of the Q-leaning algorithm where the agent begins with sufficiently high Q values but approaches correct values as it acquires penalties for wrong actions. This is an elegant solution to this particular problem because: \n",
    "\n",
    "+ of the way the rewards are structured\n",
    "+ each state is independent from each other\n",
    "+ there is only one correct action for each state\n",
    "\n",
    "As such, we only need to tune 2 parameters - learning rate alpha and our initial Q-values.  We run the simulation for a combination of learning rates and initial Q-values:\n",
    "\n",
    "+ Learning rate: 0.2 to 1.0 in increments of 0.2\n",
    "+ Q-values: 0, 2, 10\n",
    "\n",
    "The results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Alpha  Ave Penalty  Ave Reward  Efficiency  Q Initial  Success Rate\n",
      "0     0.2       -0.400      13.860   28.484841          0            48\n",
      "1     0.4       -0.385      13.535   23.987843          0            43\n",
      "2     0.6       -0.400      13.920   31.340238          0            52\n",
      "3     0.8       -0.410      11.245   22.701508          0            36\n",
      "4     1.0       -0.380      13.080   25.924206          0            48\n",
      "5     0.2       -0.490      22.250   61.520794          2           100\n",
      "6     0.4       -0.400      22.900   59.780916          2           100\n",
      "7     0.6       -0.370      22.010   60.900072          2            99\n",
      "8     0.8       -0.255      23.025   59.392063          2           100\n",
      "9     1.0       -0.285      21.535   60.724286          2           100\n",
      "10    0.2       -1.140      21.520   53.170736         10            95\n",
      "11    0.4       -0.775      21.320   54.215447         10            96\n",
      "12    0.6       -0.550      22.245   57.467482         10            99\n",
      "13    0.8       -0.480      22.395   57.158153         10            99\n",
      "14    1.0       -0.485      22.285   53.989149         10            98\n"
     ]
    }
   ],
   "source": [
    "print record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the inital Q-values are extremely important and the cab performs optimally at 2 with success rates close to 100. As for the learning rate, 0.6 to 1.0 are good values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: \n",
    "\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "    \n",
    "All in all, the cab has a very good success rate - around 99 out of 100. At best, the cab has an efficiency of 60%. That is, it only uses up 40% of the steps allotted to arrive at the destination. The average penalty per trip is around 0.3, which means it still makes a few mistakes along the way so the policy is not perfect. This is because the cab still has not encountered all possible states so it will behave naively when it is faced with a new state.\n",
    "\n",
    "An optimal policy put simply is - proceed to the next waypoint when it's safe (no violations), stay in place otherwise. We can arrive at this optimal policy by either letting the cab 'experience' all combinations of 162x4 states and actions or apply some heuristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
